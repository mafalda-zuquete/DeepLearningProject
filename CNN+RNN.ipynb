{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "\n",
    "#preprocessing\n",
    "from unidecode import unidecode\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from bs4 import BeautifulSoup\n",
    "from nltk.stem import SnowballStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "#train/test split\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "#vectorization\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "#performance metrics\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "#keras packages\n",
    "from keras import models\n",
    "from keras import layers\n",
    "import tensorflow as tf\n",
    "from keras import callbacks\n",
    "from keras.preprocessing import text\n",
    "from keras.preprocessing import sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Mafalda\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>category</th>\n",
       "      <th>headline</th>\n",
       "      <th>authors</th>\n",
       "      <th>short_description</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CRIME</td>\n",
       "      <td>There Were 2 Mass Shootings In Texas Last Week...</td>\n",
       "      <td>Melissa Jeltsen</td>\n",
       "      <td>She left her husband. He killed their children...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ENTERTAINMENT</td>\n",
       "      <td>Will Smith Joins Diplo And Nicky Jam For The 2...</td>\n",
       "      <td>Andy McDonald</td>\n",
       "      <td>Of course it has a song.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ENTERTAINMENT</td>\n",
       "      <td>Hugh Grant Marries For The First Time At Age 57</td>\n",
       "      <td>Ron Dicker</td>\n",
       "      <td>The actor and his longtime girlfriend Anna Ebe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ENTERTAINMENT</td>\n",
       "      <td>Jim Carrey Blasts 'Castrato' Adam Schiff And D...</td>\n",
       "      <td>Ron Dicker</td>\n",
       "      <td>The actor gives Dems an ass-kicking for not fi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ENTERTAINMENT</td>\n",
       "      <td>Julianna Margulies Uses Donald Trump Poop Bags...</td>\n",
       "      <td>Ron Dicker</td>\n",
       "      <td>The \"Dietland\" actress said using the bags is ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        category                                           headline  \\\n",
       "0          CRIME  There Were 2 Mass Shootings In Texas Last Week...   \n",
       "1  ENTERTAINMENT  Will Smith Joins Diplo And Nicky Jam For The 2...   \n",
       "2  ENTERTAINMENT    Hugh Grant Marries For The First Time At Age 57   \n",
       "3  ENTERTAINMENT  Jim Carrey Blasts 'Castrato' Adam Schiff And D...   \n",
       "4  ENTERTAINMENT  Julianna Margulies Uses Donald Trump Poop Bags...   \n",
       "\n",
       "           authors                                  short_description  \n",
       "0  Melissa Jeltsen  She left her husband. He killed their children...  \n",
       "1    Andy McDonald                           Of course it has a song.  \n",
       "2       Ron Dicker  The actor and his longtime girlfriend Anna Ebe...  \n",
       "3       Ron Dicker  The actor gives Dems an ass-kicking for not fi...  \n",
       "4       Ron Dicker  The \"Dietland\" actress said using the bags is ...  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Read the data\n",
    "df = pd.read_json('News_Category_Dataset_v2.json', lines = True)\n",
    "\n",
    "df = df.drop(['link', 'date'], axis = 1)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>category</th>\n",
       "      <th>headline</th>\n",
       "      <th>authors</th>\n",
       "      <th>short_description</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CRIME</td>\n",
       "      <td>There Were 2 Mass Shootings In Texas Last Week...</td>\n",
       "      <td>Melissa Jeltsen</td>\n",
       "      <td>She left her husband. He killed their children...</td>\n",
       "      <td>There Were 2 Mass Shootings In Texas Last Week...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ENTERTAINMENT</td>\n",
       "      <td>Will Smith Joins Diplo And Nicky Jam For The 2...</td>\n",
       "      <td>Andy McDonald</td>\n",
       "      <td>Of course it has a song.</td>\n",
       "      <td>Will Smith Joins Diplo And Nicky Jam For The 2...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ENTERTAINMENT</td>\n",
       "      <td>Hugh Grant Marries For The First Time At Age 57</td>\n",
       "      <td>Ron Dicker</td>\n",
       "      <td>The actor and his longtime girlfriend Anna Ebe...</td>\n",
       "      <td>Hugh Grant Marries For The First Time At Age 5...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ENTERTAINMENT</td>\n",
       "      <td>Jim Carrey Blasts 'Castrato' Adam Schiff And D...</td>\n",
       "      <td>Ron Dicker</td>\n",
       "      <td>The actor gives Dems an ass-kicking for not fi...</td>\n",
       "      <td>Jim Carrey Blasts 'Castrato' Adam Schiff And D...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ENTERTAINMENT</td>\n",
       "      <td>Julianna Margulies Uses Donald Trump Poop Bags...</td>\n",
       "      <td>Ron Dicker</td>\n",
       "      <td>The \"Dietland\" actress said using the bags is ...</td>\n",
       "      <td>Julianna Margulies Uses Donald Trump Poop Bags...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        category                                           headline  \\\n",
       "0          CRIME  There Were 2 Mass Shootings In Texas Last Week...   \n",
       "1  ENTERTAINMENT  Will Smith Joins Diplo And Nicky Jam For The 2...   \n",
       "2  ENTERTAINMENT    Hugh Grant Marries For The First Time At Age 57   \n",
       "3  ENTERTAINMENT  Jim Carrey Blasts 'Castrato' Adam Schiff And D...   \n",
       "4  ENTERTAINMENT  Julianna Margulies Uses Donald Trump Poop Bags...   \n",
       "\n",
       "           authors                                  short_description  \\\n",
       "0  Melissa Jeltsen  She left her husband. He killed their children...   \n",
       "1    Andy McDonald                           Of course it has a song.   \n",
       "2       Ron Dicker  The actor and his longtime girlfriend Anna Ebe...   \n",
       "3       Ron Dicker  The actor gives Dems an ass-kicking for not fi...   \n",
       "4       Ron Dicker  The \"Dietland\" actress said using the bags is ...   \n",
       "\n",
       "                                                text  \n",
       "0  There Were 2 Mass Shootings In Texas Last Week...  \n",
       "1  Will Smith Joins Diplo And Nicky Jam For The 2...  \n",
       "2  Hugh Grant Marries For The First Time At Age 5...  \n",
       "3  Jim Carrey Blasts 'Castrato' Adam Schiff And D...  \n",
       "4  Julianna Margulies Uses Donald Trump Poop Bags...  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['text'] = df['headline'] + df['short_description']\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing(dataframe,punctuation=False,tags=False,stemming=False,lemmatizing=False):\n",
    "    \"\"\"\n",
    "    Function that receives a Pandas DataFrame with the texts and applies\n",
    "        the chosen preprocessing techiniques.\n",
    "        \n",
    "    :param dataframe: a Pandas DataFrame in which the first column \n",
    "        contains the estracted texts the second column contains the\n",
    "        respective authors\n",
    "    :param punctuation: bool determining whether or remove punctuation\n",
    "        and numbers or not (default: False)\n",
    "    :param tags: bool determining whether to remove tags or not\n",
    "        (default: False)\n",
    "    :param stemming: bool determining whether to perform stemming or not\n",
    "        (default: False)\n",
    "    :param lemmatizing: bool determining whether to perform lemmatizing \n",
    "        or not (default: False)\n",
    "             \n",
    "    :return: Returns a list of strings which correspond to each text after\n",
    "        preprocessing\n",
    "    \"\"\"\n",
    "    \n",
    "    processed_corpus = []\n",
    "    \n",
    "    stop_words = set(stopwords.words(\"english\"))\n",
    "    \n",
    "    # for each text in the Pandas DataFrame\n",
    "    for i in tqdm(range(len(dataframe))):\n",
    "        text = dataframe[i]\n",
    "                \n",
    "        # remove punctuation\n",
    "        if punctuation:\n",
    "            text = re.sub('[^a-zA-Z]', ' ', text)\n",
    "\n",
    "        # remove tags\n",
    "        if tags:\n",
    "            text = BeautifulSoup(text).get_text()\n",
    "        \n",
    "        # convert to list from str\n",
    "        text = text.split()\n",
    "\n",
    "        # stemming\n",
    "        if stemming:\n",
    "            stemmer = SnowballStemmer('english')\n",
    "            \n",
    "            # don't stem stop words so that they can still be detected\n",
    "            text = [stemmer.stem(word) for word in text if not word in stop_words]\n",
    "            \n",
    "        if lemmatizing:\n",
    "            lemmatizer = WordNetLemmatizer()\n",
    "            \n",
    "            text = [lemmatizer.lemmatize(word) for word in text if not word in stop_words]\n",
    "        \n",
    "        # convert to str from list\n",
    "        text = \" \".join(text)\n",
    "\n",
    "        # save the preprocessed text on a list\n",
    "        processed_corpus.append(text)\n",
    "    return processed_corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7dae9083a5f54c74b9997522ed027f40",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=200853.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# preprocess the text and save it in a Pandas Series\n",
    "cleaned_text = preprocessing(\n",
    "    df['text'],\n",
    "    #punctuation=True,\n",
    "    #tags=True,\n",
    "    #stemming=True,\n",
    "    #lemmatizing=True\n",
    ")\n",
    "df['clean_text'] = pd.Series(cleaned_text, index = df.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>category</th>\n",
       "      <th>headline</th>\n",
       "      <th>authors</th>\n",
       "      <th>short_description</th>\n",
       "      <th>text</th>\n",
       "      <th>clean_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CRIME</td>\n",
       "      <td>There Were 2 Mass Shootings In Texas Last Week...</td>\n",
       "      <td>Melissa Jeltsen</td>\n",
       "      <td>She left her husband. He killed their children...</td>\n",
       "      <td>There Were 2 Mass Shootings In Texas Last Week...</td>\n",
       "      <td>There Were 2 Mass Shootings In Texas Last Week...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ENTERTAINMENT</td>\n",
       "      <td>Will Smith Joins Diplo And Nicky Jam For The 2...</td>\n",
       "      <td>Andy McDonald</td>\n",
       "      <td>Of course it has a song.</td>\n",
       "      <td>Will Smith Joins Diplo And Nicky Jam For The 2...</td>\n",
       "      <td>Will Smith Joins Diplo And Nicky Jam For The 2...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ENTERTAINMENT</td>\n",
       "      <td>Hugh Grant Marries For The First Time At Age 57</td>\n",
       "      <td>Ron Dicker</td>\n",
       "      <td>The actor and his longtime girlfriend Anna Ebe...</td>\n",
       "      <td>Hugh Grant Marries For The First Time At Age 5...</td>\n",
       "      <td>Hugh Grant Marries For The First Time At Age 5...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ENTERTAINMENT</td>\n",
       "      <td>Jim Carrey Blasts 'Castrato' Adam Schiff And D...</td>\n",
       "      <td>Ron Dicker</td>\n",
       "      <td>The actor gives Dems an ass-kicking for not fi...</td>\n",
       "      <td>Jim Carrey Blasts 'Castrato' Adam Schiff And D...</td>\n",
       "      <td>Jim Carrey Blasts 'Castrato' Adam Schiff And D...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ENTERTAINMENT</td>\n",
       "      <td>Julianna Margulies Uses Donald Trump Poop Bags...</td>\n",
       "      <td>Ron Dicker</td>\n",
       "      <td>The \"Dietland\" actress said using the bags is ...</td>\n",
       "      <td>Julianna Margulies Uses Donald Trump Poop Bags...</td>\n",
       "      <td>Julianna Margulies Uses Donald Trump Poop Bags...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        category                                           headline  \\\n",
       "0          CRIME  There Were 2 Mass Shootings In Texas Last Week...   \n",
       "1  ENTERTAINMENT  Will Smith Joins Diplo And Nicky Jam For The 2...   \n",
       "2  ENTERTAINMENT    Hugh Grant Marries For The First Time At Age 57   \n",
       "3  ENTERTAINMENT  Jim Carrey Blasts 'Castrato' Adam Schiff And D...   \n",
       "4  ENTERTAINMENT  Julianna Margulies Uses Donald Trump Poop Bags...   \n",
       "\n",
       "           authors                                  short_description  \\\n",
       "0  Melissa Jeltsen  She left her husband. He killed their children...   \n",
       "1    Andy McDonald                           Of course it has a song.   \n",
       "2       Ron Dicker  The actor and his longtime girlfriend Anna Ebe...   \n",
       "3       Ron Dicker  The actor gives Dems an ass-kicking for not fi...   \n",
       "4       Ron Dicker  The \"Dietland\" actress said using the bags is ...   \n",
       "\n",
       "                                                text  \\\n",
       "0  There Were 2 Mass Shootings In Texas Last Week...   \n",
       "1  Will Smith Joins Diplo And Nicky Jam For The 2...   \n",
       "2  Hugh Grant Marries For The First Time At Age 5...   \n",
       "3  Jim Carrey Blasts 'Castrato' Adam Schiff And D...   \n",
       "4  Julianna Margulies Uses Donald Trump Poop Bags...   \n",
       "\n",
       "                                          clean_text  \n",
       "0  There Were 2 Mass Shootings In Texas Last Week...  \n",
       "1  Will Smith Joins Diplo And Nicky Jam For The 2...  \n",
       "2  Hugh Grant Marries For The First Time At Age 5...  \n",
       "3  Jim Carrey Blasts 'Castrato' Adam Schiff And D...  \n",
       "4  Julianna Margulies Uses Donald Trump Poop Bags...  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = df['clean_text']\n",
    "target = df['category']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulary_size = 20000\n",
    "tokenizer = text.Tokenizer(num_words = vocabulary_size)\n",
    "tokenizer.fit_on_texts(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequences = tokenizer.texts_to_sequences(data)\n",
    "len_sequences = [len(x) for x in sequences]\n",
    "max_len = max(len_sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "241"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = sequence.pad_sequences(sequences, maxlen=max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train,X,y_train,y = train_test_split(data,target,test_size=0.4,shuffle=True,stratify=target,random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_val,X_test,y_val,y_test = train_test_split(X,y,test_size=0.5,shuffle=True,stratify=y,random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train.reshape((X_train.shape[0],X_train.shape[1],1))\n",
    "X_val = X_val.reshape((X_val.shape[0],X_val.shape[1],1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(120511, 241, 1)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = y_train.values.reshape((120511,1))\n",
    "y_val = y_val.values.reshape((40171,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "enc = OneHotEncoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = enc.fit_transform(y_train)\n",
    "y_val = enc.transform(y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nn(nodes=64,window=5,activation='relu',pooling=3,optimizer='rmsprop',loss='categorical_crossentropy'):\n",
    "    np.random.seed(1)\n",
    "    tf.random.set_seed(2)\n",
    "    model = models.Sequential()\n",
    "    model.add(layers.Conv1D(nodes,window,activation=activation,input_shape=(None,X_train.shape[-1])))\n",
    "    model.add(layers.MaxPooling1D(pooling))\n",
    "    #model.add(layers.Conv1D(nodes,window,activation=activation))\n",
    "    model.add(layers.GRU(nodes))#,\n",
    "                         #return_sequences=True))\n",
    "                         #dropout=0.2,\n",
    "                         #recurrent_dropout=0.2))\n",
    "    #model.add(layers.GRU(nodes))\n",
    "    model.add(layers.Dense(41, activation='softmax'))\n",
    "    model.compile(loss=loss, optimizer=optimizer, metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = nn()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_1 (Conv1D)            (None, None, 64)          384       \n",
      "_________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1 (None, None, 64)          0         \n",
      "_________________________________________________________________\n",
      "gru_1 (GRU)                  (None, 64)                24768     \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 41)                2665      \n",
      "=================================================================\n",
      "Total params: 27,817\n",
      "Trainable params: 27,817\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "callbacks_list = [\n",
    "    callbacks.EarlyStopping(\n",
    "        monitor = 'val_accuracy',\n",
    "        patience = 10\n",
    "    ),\n",
    "    callbacks.ModelCheckpoint(\n",
    "        filepath = 'nn.h5',\n",
    "        monitor = 'val_accuracy',\n",
    "        save_best_only = True\n",
    "    )\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 120511 samples, validate on 40171 samples\n",
      "Epoch 1/100\n",
      "120511/120511 [==============================] - 124s 1ms/step - loss: 3.3203 - accuracy: 0.1595 - val_loss: 3.2652 - val_accuracy: 0.1646\n",
      "Epoch 2/100\n",
      "120511/120511 [==============================] - 133s 1ms/step - loss: 3.2552 - accuracy: 0.1694 - val_loss: 3.2561 - val_accuracy: 0.1695\n",
      "Epoch 3/100\n",
      "120511/120511 [==============================] - 133s 1ms/step - loss: 3.2395 - accuracy: 0.1734 - val_loss: 3.2365 - val_accuracy: 0.1753\n",
      "Epoch 4/100\n",
      "120511/120511 [==============================] - 137s 1ms/step - loss: 3.2252 - accuracy: 0.1757 - val_loss: 3.2170 - val_accuracy: 0.1780\n",
      "Epoch 5/100\n",
      "120511/120511 [==============================] - 141s 1ms/step - loss: 3.2120 - accuracy: 0.1769 - val_loss: 3.2191 - val_accuracy: 0.1692\n",
      "Epoch 6/100\n",
      "120511/120511 [==============================] - 146s 1ms/step - loss: 3.2151 - accuracy: 0.1749 - val_loss: 3.2140 - val_accuracy: 0.1751\n",
      "Epoch 7/100\n",
      "120511/120511 [==============================] - 136s 1ms/step - loss: 3.2157 - accuracy: 0.1750 - val_loss: 3.2081 - val_accuracy: 0.1803\n",
      "Epoch 8/100\n",
      "120511/120511 [==============================] - 133s 1ms/step - loss: 3.2001 - accuracy: 0.1769 - val_loss: 3.2070 - val_accuracy: 0.1772\n",
      "Epoch 9/100\n",
      "120511/120511 [==============================] - 132s 1ms/step - loss: 3.2004 - accuracy: 0.1770 - val_loss: 3.1930 - val_accuracy: 0.1778\n",
      "Epoch 10/100\n",
      "120511/120511 [==============================] - 126s 1ms/step - loss: 3.1937 - accuracy: 0.1781 - val_loss: 3.2013 - val_accuracy: 0.1802\n",
      "Epoch 11/100\n",
      "120511/120511 [==============================] - 126s 1ms/step - loss: 3.2058 - accuracy: 0.1770 - val_loss: 3.2044 - val_accuracy: 0.1805\n",
      "Epoch 12/100\n",
      "120511/120511 [==============================] - 132s 1ms/step - loss: 3.1914 - accuracy: 0.1811 - val_loss: 3.1815 - val_accuracy: 0.1855\n",
      "Epoch 13/100\n",
      "120511/120511 [==============================] - 123s 1ms/step - loss: 3.1825 - accuracy: 0.1813 - val_loss: 3.1855 - val_accuracy: 0.1768\n",
      "Epoch 14/100\n",
      "120511/120511 [==============================] - 128s 1ms/step - loss: 3.1785 - accuracy: 0.1807 - val_loss: 3.2190 - val_accuracy: 0.1658\n",
      "Epoch 15/100\n",
      "120511/120511 [==============================] - 131s 1ms/step - loss: 3.1741 - accuracy: 0.1823 - val_loss: 3.1744 - val_accuracy: 0.1869\n",
      "Epoch 16/100\n",
      "120511/120511 [==============================] - 130s 1ms/step - loss: 3.1703 - accuracy: 0.1837 - val_loss: 3.1799 - val_accuracy: 0.1817\n",
      "Epoch 17/100\n",
      "120511/120511 [==============================] - 137s 1ms/step - loss: 3.1730 - accuracy: 0.1835 - val_loss: 3.1766 - val_accuracy: 0.1837\n",
      "Epoch 18/100\n",
      "120511/120511 [==============================] - 136s 1ms/step - loss: 3.1656 - accuracy: 0.1829 - val_loss: 3.1579 - val_accuracy: 0.1893\n",
      "Epoch 19/100\n",
      "120511/120511 [==============================] - 130s 1ms/step - loss: 3.1587 - accuracy: 0.1840 - val_loss: 3.1495 - val_accuracy: 0.1871\n",
      "Epoch 20/100\n",
      "120511/120511 [==============================] - 126s 1ms/step - loss: 3.1577 - accuracy: 0.1849 - val_loss: 3.2202 - val_accuracy: 0.1755\n",
      "Epoch 21/100\n",
      "120511/120511 [==============================] - 133s 1ms/step - loss: 3.1538 - accuracy: 0.1849 - val_loss: 3.1499 - val_accuracy: 0.1842\n",
      "Epoch 22/100\n",
      "120511/120511 [==============================] - 132s 1ms/step - loss: 3.1462 - accuracy: 0.1865 - val_loss: 3.1378 - val_accuracy: 0.1880\n",
      "Epoch 23/100\n",
      "120511/120511 [==============================] - 136s 1ms/step - loss: 3.1431 - accuracy: 0.1878 - val_loss: 3.1377 - val_accuracy: 0.1867\n",
      "Epoch 24/100\n",
      "120511/120511 [==============================] - 135s 1ms/step - loss: 3.1346 - accuracy: 0.1871 - val_loss: 3.1188 - val_accuracy: 0.1901\n",
      "Epoch 25/100\n",
      "120511/120511 [==============================] - 136s 1ms/step - loss: 3.1313 - accuracy: 0.1884 - val_loss: 3.1190 - val_accuracy: 0.1894\n",
      "Epoch 26/100\n",
      "120511/120511 [==============================] - 134s 1ms/step - loss: 3.1356 - accuracy: 0.1877 - val_loss: 3.1484 - val_accuracy: 0.1849\n",
      "Epoch 27/100\n",
      "120511/120511 [==============================] - 135s 1ms/step - loss: 3.1349 - accuracy: 0.1871 - val_loss: 3.2097 - val_accuracy: 0.1766\n",
      "Epoch 28/100\n",
      "120511/120511 [==============================] - 127s 1ms/step - loss: 3.1331 - accuracy: 0.1879 - val_loss: 3.1526 - val_accuracy: 0.1863\n",
      "Epoch 29/100\n",
      "120511/120511 [==============================] - 141s 1ms/step - loss: 3.1314 - accuracy: 0.1880 - val_loss: 3.1123 - val_accuracy: 0.1907\n",
      "Epoch 30/100\n",
      "120511/120511 [==============================] - 133s 1ms/step - loss: 3.1232 - accuracy: 0.1899 - val_loss: 3.1058 - val_accuracy: 0.1918\n",
      "Epoch 31/100\n",
      "120511/120511 [==============================] - 145s 1ms/step - loss: 3.1263 - accuracy: 0.1880 - val_loss: 3.1549 - val_accuracy: 0.1861\n",
      "Epoch 32/100\n",
      "120511/120511 [==============================] - 145s 1ms/step - loss: 3.1204 - accuracy: 0.1894 - val_loss: 3.2104 - val_accuracy: 0.1786\n",
      "Epoch 33/100\n",
      "120511/120511 [==============================] - 144s 1ms/step - loss: 3.1198 - accuracy: 0.1897 - val_loss: 3.1190 - val_accuracy: 0.1841\n",
      "Epoch 34/100\n",
      "120511/120511 [==============================] - 139s 1ms/step - loss: 3.1162 - accuracy: 0.1907 - val_loss: 3.0857 - val_accuracy: 0.1975\n",
      "Epoch 35/100\n",
      "120511/120511 [==============================] - 130s 1ms/step - loss: 3.1193 - accuracy: 0.1899 - val_loss: 3.1680 - val_accuracy: 0.1841\n",
      "Epoch 36/100\n",
      "120511/120511 [==============================] - 129s 1ms/step - loss: 3.1163 - accuracy: 0.1901 - val_loss: 3.1329 - val_accuracy: 0.1911\n",
      "Epoch 37/100\n",
      "120511/120511 [==============================] - 129s 1ms/step - loss: 3.1234 - accuracy: 0.1882 - val_loss: 3.1234 - val_accuracy: 0.1871\n",
      "Epoch 38/100\n",
      "120511/120511 [==============================] - 143s 1ms/step - loss: 3.1194 - accuracy: 0.1890 - val_loss: 3.1107 - val_accuracy: 0.1918\n",
      "Epoch 39/100\n",
      "120511/120511 [==============================] - 149s 1ms/step - loss: 3.1208 - accuracy: 0.1888 - val_loss: 3.2015 - val_accuracy: 0.1777\n",
      "Epoch 40/100\n",
      "120511/120511 [==============================] - 128s 1ms/step - loss: 3.1129 - accuracy: 0.1908 - val_loss: 3.0984 - val_accuracy: 0.1942\n",
      "Epoch 41/100\n",
      "120511/120511 [==============================] - 130s 1ms/step - loss: 3.1140 - accuracy: 0.1905 - val_loss: 3.1715 - val_accuracy: 0.1738\n",
      "Epoch 42/100\n",
      "120511/120511 [==============================] - 134s 1ms/step - loss: 3.1138 - accuracy: 0.1902 - val_loss: 3.0937 - val_accuracy: 0.1934\n",
      "Epoch 43/100\n",
      "120511/120511 [==============================] - 136s 1ms/step - loss: 3.1094 - accuracy: 0.1903 - val_loss: 3.1060 - val_accuracy: 0.1931\n",
      "Epoch 44/100\n",
      "120511/120511 [==============================] - 142s 1ms/step - loss: 3.1105 - accuracy: 0.1910 - val_loss: 3.1378 - val_accuracy: 0.1892\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(X_train,y_train,epochs=100,batch_size=512,callbacks=callbacks_list,validation_data=(X_val,y_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
