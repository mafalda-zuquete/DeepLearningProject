{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning Project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Catarina Palha (M20190156)\n",
    "\n",
    "Mafalda ZÃºquete (M20190257)\n",
    "\n",
    "Maren Leuthner (M20190134)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load necessary packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "\n",
    "#preprocessing\n",
    "from unidecode import unidecode\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from bs4 import BeautifulSoup\n",
    "from nltk.stem import SnowballStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "#train/test split\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "#vectorization\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "#keras packages\n",
    "from keras import models\n",
    "from keras import layers\n",
    "import tensorflow as tf\n",
    "from keras import callbacks\n",
    "from keras.preprocessing import text\n",
    "from keras.preprocessing import sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Read the data\n",
    "df = pd.read_json('News_Category_Dataset_v2.json', lines = True)\n",
    "\n",
    "df = df.drop(['link', 'date'], axis = 1)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['text'] = df['headline'] + df['short_description']\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check null values\n",
    "print(df.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_count = df['text'].apply(lambda x: len(str(x).split(\" \")))\n",
    "df['word_count'] = word_count\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It can be seen that the size of the documents is homogenous."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['word_count'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupby(['category']).sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It can be seen that some categories are undersampled."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_words = ' '.join(df['test']).split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the frequency of the words all over the headlines\n",
    "freq = pd.Series(all_words).value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "freq[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words(\"english\"))\n",
    "count = 0\n",
    "\n",
    "# count the frequent words which are also stop words\n",
    "for word in freq.index[:20]:\n",
    "    if word in stop_words:\n",
    "        count += 1\n",
    "count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The most common words are also stop words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_top_n_grams(corpus, top_k, n):\n",
    "    \"\"\"\n",
    "    Function that receives a list of documents (corpus) and extracts\n",
    "        the top k most frequent n-grams for that corpus.\n",
    "        \n",
    "    :param corpus: list of texts\n",
    "    :param top_k: int with the number of n-grams that we want to extract\n",
    "    :param n: n gram type to be considered \n",
    "             (if n=1 extracts unigrams, if n=2 extracts bigrams, ...)\n",
    "             \n",
    "    :return: Returns a sorted Pandas DataFrame in which the first column \n",
    "        contains the extracted ngrams and the second column contains\n",
    "        the respective counts\n",
    "    \"\"\"\n",
    "    # get the top 2000 n-grams\n",
    "    vec = CountVectorizer(ngram_range=(n, n), max_features=2000).fit(corpus)\n",
    "    \n",
    "    bag_of_words = vec.transform(corpus)\n",
    "    \n",
    "    # count how many times a word appears in the corpora\n",
    "    sum_words = bag_of_words.sum(axis=0) \n",
    "    \n",
    "    words_freq = []\n",
    "    for word, idx in vec.vocabulary_.items():\n",
    "        words_freq.append((word, sum_words[0, idx]))\n",
    "        \n",
    "    # save the frequencies in a Pandas DataFrame\n",
    "    words_freq = sorted(words_freq, key=lambda x: x[1], reverse=True)\n",
    "    top_df = pd.DataFrame(words_freq[:top_k])\n",
    "    top_df.columns = [\"Ngram\", \"Freq\"]\n",
    "    return top_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_frequencies(top_df):\n",
    "    \"\"\"\n",
    "    Function that receives a Pandas DataFrame from the \"get_top_n_grams\" function\n",
    "        and plots the frequencies in a bar plot.\n",
    "        \n",
    "    :param top_df: a sorted Pandas DataFrame in which the first column \n",
    "        contains the top k ngrams and the second column contains\n",
    "        the respective counts\n",
    "    \"\"\"\n",
    "    x_labels = top_df[\"Ngram\"][:30]\n",
    "    y_pos = np.arange(len(x_labels))\n",
    "    values = top_df[\"Freq\"][:30]\n",
    "    plt.bar(y_pos, values, align='center', alpha=0.5)\n",
    "    plt.xticks(y_pos, x_labels)\n",
    "    plt.ylabel('Frequencies')\n",
    "    plt.title('Words')\n",
    "    plt.xticks(rotation=90)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the distribution of the top 20 n-grams with n up to 5\n",
    "for i in range(1,6):\n",
    "    top = get_top_n_grams(df[\"test\"], top_k=20, n=i)\n",
    "    plot_frequencies(top)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing(dataframe,punctuation=False,tags=False,stemming=False,lemmatizing=False,stopWords=False,\n",
    "                  lowercasing=False,accents=False):\n",
    "    \"\"\"\n",
    "    Function that receives a Pandas DataFrame with the texts and applies\n",
    "        the chosen preprocessing techiniques.\n",
    "        \n",
    "    :param dataframe: a Pandas DataFrame in which the first column \n",
    "        contains the estracted texts the second column contains the\n",
    "        respective authors\n",
    "    :param punctuation: bool determining whether or remove punctuation\n",
    "        and numbers or not (default: False)\n",
    "    :param tags: bool determining whether to remove tags or not\n",
    "        (default: False)\n",
    "    :param stemming: bool determining whether to perform stemming or not\n",
    "        (default: False)\n",
    "    :param lemmatizing: bool determining whether to perform lemmatizing \n",
    "        or not (default: False)\n",
    "             \n",
    "    :return: Returns a list of strings which correspond to each text after\n",
    "        preprocessing\n",
    "    \"\"\"\n",
    "    \n",
    "    processed_corpus = []\n",
    "    \n",
    "    stop_words = set(stopwords.words(\"english\"))\n",
    "    \n",
    "    # for each text in the Pandas DataFrame\n",
    "    for i in tqdm(range(len(dataframe))):\n",
    "        text = dataframe[i]\n",
    "                \n",
    "        # remove punctuation\n",
    "        if punctuation:\n",
    "            text = re.sub('[^a-zA-Z]', ' ', text)\n",
    "\n",
    "        # remove tags\n",
    "        if tags:\n",
    "            text = BeautifulSoup(text).get_text()\n",
    "        \n",
    "        # convert to list from str\n",
    "        text = text.split()\n",
    "\n",
    "        # stemming\n",
    "        if stemming:\n",
    "            stemmer = SnowballStemmer('english')\n",
    "            \n",
    "            # don't stem stop words so that they can still be detected\n",
    "            text = [stemmer.stem(word) for word in text if not word in stop_words]\n",
    "        \n",
    "        # lemmatization\n",
    "        if lemmatizing:\n",
    "            lemmatizer = WordNetLemmatizer()\n",
    "            \n",
    "            text = [lemmatizer.lemmatize(word) for word in text if not word in stop_words]\n",
    "        \n",
    "        # removing stop words\n",
    "        if stopWords:\n",
    "            text = [word for word in text if not word in stop_words]\n",
    "        \n",
    "        # convert to str from list\n",
    "        text = \" \".join(text)\n",
    "        \n",
    "        # lowecase the text\n",
    "        if lowercasing:\n",
    "            text = text.lower()\n",
    "        \n",
    "        # remove accents\n",
    "        if accents:\n",
    "            text = unidecode(text)\n",
    "\n",
    "        # save the preprocessed text on a list\n",
    "        processed_corpus.append(text)\n",
    "    return processed_corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocess the text and save it in a Pandas Series\n",
    "cleaned_text = preprocessing(\n",
    "    df['text'],\n",
    "    #punctuation=True,\n",
    "    #tags=True,\n",
    "    #stemming=True,\n",
    "    #lemmatizing=True\n",
    ")\n",
    "df['clean_text'] = pd.Series(cleaned_text, index = df.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = df['clean_text']\n",
    "target = df['category']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train,X,y_train,y = train_test_split(data,target,test_size=0.4,shuffle=True,stratify=target,random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_val,X_test,y_val,y_test = train_test_split(X,y,test_size=0.5,shuffle=True,stratify=y,random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = y_train.values.reshape((120511,1))\n",
    "y_val = y_val.values.reshape((40171,1))\n",
    "y_test = y_test.values.reshape((40171,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = enc.fit_transform(y_train)\n",
    "y_val = enc.transform(y_val)\n",
    "y_test = enc.transform(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "callbacks_list = [\n",
    "    callbacks.EarlyStopping(\n",
    "        monitor = 'val_accuracy',\n",
    "        patience = 1\n",
    "    )\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(\n",
    "    #max_df=0.6,\n",
    "    #strip_accents='unicode',\n",
    "    lowercase=False,\n",
    "    #stop_words=stop_words,\n",
    "    max_features=157533,\n",
    "    ngram_range=(1,2)\n",
    ")\n",
    "\n",
    "enc = OneHotEncoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_dnn = vectorizer.fit_transform(X_train)\n",
    "X_val_dnn = vectorizer.transform(X_val)\n",
    "X_test_dnn = vectorizer.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dnn(nodes=64,activation='relu',optimizer='rmsprop',loss='categorical_crossentropy'):\n",
    "    np.random.seed(1)\n",
    "    tf.random.set_seed(2)\n",
    "    model = models.Sequential()\n",
    "    model.add(layers.Dense(nodes,activation=activation,input_shape=(X_train.shape[1],)))\n",
    "    model.add(layers.Dropout(0.2))\n",
    "    #model.add(layers.Dense(nodes,activation=activation))\n",
    "    #model.add(layers.Dropout(0.2))\n",
    "    #model.add(layers.Dense(nodes,kernel_regularizer=regularizers.l2(0.001),activation=activation))\n",
    "    model.add(layers.Dense(41,activation='softmax'))\n",
    "    model.compile(optimizer=optimizer,loss=loss,metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dnn = dnn(nodes=250)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history_dnn = model_dnn.fit(X_train_dnn,y_train,epochs=100,batch_size=512,callbacks=callbacks_list,\n",
    "                            validation_data=(X_val_dnn,y_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#vocabulary_size = 20000\n",
    "vocabulary_size = 15000\n",
    "tokenizer = text.Tokenizer(num_words = vocabulary_size)\n",
    "tokenizer.fit_on_texts(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequences_train = tokenizer.texts_to_sequences(X_train)\n",
    "sequences_val = tokenizer.texts_to_sequences(X_val)\n",
    "sequences_test = tokenizer.texts_to_sequences(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len_sequences = [len(x) for x in sequences_train]\n",
    "max_len = max(len_sequences)\n",
    "max_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_rnn = sequence.pad_sequences(sequences_train, maxlen=max_len)\n",
    "X_val_rnn = sequence.pad_sequences(sequences_val, maxlen=max_len)\n",
    "X_test_rnn = sequence.pad_sequences(sequences_test, maxlen=max_len)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### GRU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gru(nodes=64,optimizer='rmsprop',loss='categorical_crossentropy'):\n",
    "    np.random.seed(1)\n",
    "    tf.random.set_seed(2)\n",
    "    model = models.Sequential()\n",
    "    model.add(layers.Embedding(vocabulary_size, nodes, input_length=150))\n",
    "    model.add(layers.GRU(nodes,\n",
    "                         #dropout=0.5,\n",
    "                         #recurrent_dropout=0.5,\n",
    "                         input_shape=(None,X_train.shape[-1]),\n",
    "                         #return_sequences=True\n",
    "                        ))\n",
    "    model.add(layers.Dense(41, activation='softmax'))\n",
    "    model.compile(loss=loss, optimizer=optimizer, metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_gru = gru()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history_gru = model_gru.fit(X_train_rnn,y_train,epochs=100,batch_size=256,callbacks=callbacks_list,\n",
    "                            validation_data=(X_val_rnn,y_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lstm(nodes=64,optimizer='rmsprop',loss='categorical_crossentropy'):\n",
    "    np.random.seed(1)\n",
    "    tf.random.set_seed(2)\n",
    "    model = models.Sequential()\n",
    "    model.add(layers.Embedding(vocabulary_size, nodes, input_length=max_len))\n",
    "    model.add(layers.LSTM(nodes,\n",
    "                          #dropout=0.2,\n",
    "                          #recurrent_dropout=0.2\n",
    "                         ))\n",
    "    model.add(layers.Dense(41, activation='softmax'))\n",
    "    model.compile(loss=loss, optimizer=optimizer, metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_lstm = lstm()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history_lstm = model_lstm.fit(X_train_rnn,y_train,epochs=100,batch_size=512,callbacks=callbacks_list,\n",
    "                              validation_data=(X_val_rnn,y_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CNN + GRU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cnn_gru(nodes=64,window=5,activation='relu',pooling=3,optimizer='rmsprop',loss='categorical_crossentropy'):\n",
    "    np.random.seed(1)\n",
    "    tf.random.set_seed(2)\n",
    "    model = models.Sequential()\n",
    "    model.add(layers.Conv1D(nodes,window,activation=activation,input_shape=(None,X_train.shape[-1])))\n",
    "    model.add(layers.MaxPooling1D(pooling))\n",
    "    model.add(layers.GRU(nodes))#,\n",
    "                         #return_sequences=True))\n",
    "                         #dropout=0.2,\n",
    "                         #recurrent_dropout=0.2))\n",
    "    model.add(layers.Dense(41, activation='softmax'))\n",
    "    model.compile(loss=loss, optimizer=optimizer, metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_cnn_gru = cnn_gru()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history_cnn_gru = model_cnn_gru.fit(X_train_rnn,y_train,epochs=100,batch_size=512,callbacks=callbacks_list,\n",
    "                                    validation_data=(X_val_rnn,y_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CNN + LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cnn_lstm(nodes=64,window=5,activation='relu',pooling=3,optimizer='rmsprop',loss='categorical_crossentropy'):\n",
    "    np.random.seed(1)\n",
    "    tf.random.set_seed(2)\n",
    "    model = models.Sequential()\n",
    "    model.add(layers.Conv1D(nodes,window,activation=activation,input_shape=(None,X_train.shape[-1])))\n",
    "    model.add(layers.MaxPooling1D(pooling))\n",
    "    model.add(layers.LSTM(nodes))#,\n",
    "                         #return_sequences=True))\n",
    "                         #dropout=0.2,\n",
    "                         #recurrent_dropout=0.2))\n",
    "    model.add(layers.Dense(41, activation='softmax'))\n",
    "    model.compile(loss=loss, optimizer=optimizer, metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_cnn_lstm = cnn_lstm()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history_cnn_lstm = model_cnn_lstm.fit(X_train_rnn,y_train,epochs=100,batch_size=512,callbacks=callbacks_list,\n",
    "                                      validation_data=(X_val_rnn,y_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the chosen models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "callbacks_list_dnn = [\n",
    "    callbacks.EarlyStopping(\n",
    "        monitor = 'val_accuracy',\n",
    "        patience = 10\n",
    "    ),\n",
    "    callbacks.ModelCheckpoint(\n",
    "        filepath = 'dnn.h5',\n",
    "        monitor = 'val_accuracy',\n",
    "        save_best_only = True\n",
    "    )\n",
    "]\n",
    "\n",
    "callbacks_list_rnn = [\n",
    "    callbacks.EarlyStopping(\n",
    "        monitor = 'val_accuracy',\n",
    "        patience = 10\n",
    "    ),\n",
    "    callbacks.ModelCheckpoint(\n",
    "        filepath = 'rnn.h5',\n",
    "        monitor = 'val_accuracy',\n",
    "        save_best_only = True\n",
    "    )\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history_dnn = model_dnn.fit(X_train_dnn,y_train,epochs=100,batch_size=512,callbacks=callbacks_list_dnn,\n",
    "                            validation_data=(X_val_dnn,y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history_gru = model_gru.fit(X_train_rnn,y_train,epochs=100,batch_size=256,callbacks=callbacks_list_rnn,\n",
    "                            validation_data=(X_val_rnn,y_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classify new data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dnn.load_weights('dnn.h5')\n",
    "model_gru.load_weights('rnn.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dnn.evaluate(X_test_dnn,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_gru.evaluate(X_test_rnn,y_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
